{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kmeans_w_transfer_learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Information"
      ],
      "metadata": {
        "id": "xju3b9S8jD8p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform unsupervised image clustering using a pre-trained VGG16 model (transfer learning from imagenet weights) to cluster apples vs cabbages"
      ],
      "metadata": {
        "id": "1cSGqGOcjGDb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References"
      ],
      "metadata": {
        "id": "L9RNHLjxhqig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- [Image clustering using Transfer learning](https://towardsdatascience.com/image-clustering-using-transfer-learning-df5862779571)\n",
        "- [Clustering](https://ml-with-tensorflow.info/2017/03/11/clustering/)\n",
        "- [Image Clustering Using k-Means](https://towardsdatascience.com/image-clustering-using-k-means-4a78478d2b83)"
      ],
      "metadata": {
        "id": "KeLnXXlKhy79"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "pha32_ZDhmIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import pathlib\n",
        "import PIL\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import shutil\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from sklearn.cluster import KMeans"
      ],
      "metadata": {
        "id": "51CfnxfDEik1"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "# Parameters for Deep Learning\n",
        "IMAGE_HEIGHT=224\n",
        "IMAGE_WIDTH=224\n",
        "\n",
        "# Test / Code constants\n",
        "DATASET_URL='https://github.com/rencete/computer-vision-datasets/raw/master/apple-cabbage/Dataset.7z'\n",
        "TRAIN_DATASET_BASEPATH='Dataset/train'\n",
        "TEST_DATASET_BASEPATH='Dataset/test'\n",
        "CLASSES=['apple', 'cabbage']\n",
        "CLUSTERED_OUTPUT_DIR_BASEPATH='Output'\n",
        "CLUSTERED_OUTPUT_ARCHIVE_BASEFILENAME='clustered_outputs'"
      ],
      "metadata": {
        "id": "AyJyMPz4Edy1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download and Extract Images for clustering"
      ],
      "metadata": {
        "id": "EV9CvXdRERDv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_Y_IjZZbEPR4"
      },
      "outputs": [],
      "source": [
        "# Retrieve the filename of the dataset from the URL\n",
        "dataset_filename = DATASET_URL.split('/')[-1]\n",
        "\n",
        "# Download the dataset from Github and save in current directory\n",
        "if not os.path.isdir(dataset_filename):\n",
        "  with open(dataset_filename,'wb') as f:\n",
        "    f.write(requests.get(DATASET_URL).content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the dataset has been uncompressed by checking if directory exists\n",
        "if not os.path.isdir(TRAIN_DATASET_BASEPATH):\n",
        "  !7z x Dataset.7z"
      ],
      "metadata": {
        "id": "3AnUqJvdEpT9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "nP1ose7eGEQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Load the images from the files and store as a dataset.\n",
        "Also generates the labes for the dataset as: 0 - apple, 1 - cabbage.\n",
        "'''\n",
        "def load_ds_from_path(base_path: str, augment: bool = True):\n",
        "  X = None\n",
        "\n",
        "  # Iterate through the different classes\n",
        "  for i, c in enumerate(CLASSES):\n",
        "    path = pathlib.Path(f'{base_path}/{c}')\n",
        "\n",
        "    # Load images from files\n",
        "    ds = np.array([img_to_array(load_img(p, color_mode='rgb', target_size=(IMAGE_HEIGHT, IMAGE_WIDTH))) for p in path.glob('*.jpg')])\n",
        "\n",
        "    # Concatenate / merge datasets for output\n",
        "    if X is None: \n",
        "      # initial case, no data to concatenate yet\n",
        "      X = ds\n",
        "    else:\n",
        "      # concatenate with previously loaded/augmented data\n",
        "      X = np.concatenate((X, ds))\n",
        "\n",
        "  return X"
      ],
      "metadata": {
        "id": "dSDy9KLeE6Nc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_images_for_clustering():\n",
        "  # Get images from both train and test folders\n",
        "  train_ds = load_ds_from_path(TRAIN_DATASET_BASEPATH)\n",
        "  test_ds = load_ds_from_path(TEST_DATASET_BASEPATH)\n",
        "\n",
        "  # Merge both train and test into 1 massive batch for clustering\n",
        "  ds = np.concatenate((train_ds, test_ds))\n",
        "\n",
        "  # Shuffle order\n",
        "  X = tf.random.shuffle(ds)\n",
        "\n",
        "  return X"
      ],
      "metadata": {
        "id": "RYI7-pSrIVjz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_features_from_vgg16(images):\n",
        "  # Prepare pre-trained VGG16 model\n",
        "  model = VGG16(include_top=False, weights='imagenet', pooling='avg')\n",
        "  # Model layer weights should not be trainable\n",
        "  for layer in model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "  # Preprocess images\n",
        "  X = preprocess_input(images)\n",
        "\n",
        "  return model.predict(images)  "
      ],
      "metadata": {
        "id": "jdm5KBB3HvO6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cluster_features(features):\n",
        "  # Use K-Means clustering to cluster the image features\n",
        "  kmeans = KMeans(n_clusters=2).fit(features)\n",
        "  return kmeans.labels_"
      ],
      "metadata": {
        "id": "8a3UlTnBWlgG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_clustered_images(images, labels):\n",
        "  # Create directories if it does not yet exist\n",
        "  if not os.path.exists(CLUSTERED_OUTPUT_DIR_BASEPATH):\n",
        "    os.makedirs(CLUSTERED_OUTPUT_DIR_BASEPATH)\n",
        "  for c in ['0', '1']:\n",
        "    if not os.path.exists(os.path.join(CLUSTERED_OUTPUT_DIR_BASEPATH, c)):\n",
        "      os.makedirs(os.path.join(CLUSTERED_OUTPUT_DIR_BASEPATH, c))\n",
        "\n",
        "  # Delete existing files (in case ran multiple times)\n",
        "  for root, dirs, files in os.walk(CLUSTERED_OUTPUT_DIR_BASEPATH):\n",
        "    for file in files:\n",
        "        os.remove(os.path.join(root, file))\n",
        "  \n",
        "  # Save clustered images\n",
        "  for i in range(images.shape[0]):\n",
        "    img = tf.keras.utils.array_to_img(images[i])\n",
        "    img.save(os.path.join(CLUSTERED_OUTPUT_DIR_BASEPATH, str(labels[i]), f'{str(i)}.jpg'))\n",
        "\n",
        "  # Zip directory for downloading\n",
        "  shutil.make_archive(CLUSTERED_OUTPUT_ARCHIVE_BASEFILENAME, 'zip', CLUSTERED_OUTPUT_DIR_BASEPATH)"
      ],
      "metadata": {
        "id": "AsHoqrnLcOXn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Actual Code Run"
      ],
      "metadata": {
        "id": "zdqnsMKfhgMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images = get_images_for_clustering()"
      ],
      "metadata": {
        "id": "_le8lJRDW0So"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = get_features_from_vgg16(images)"
      ],
      "metadata": {
        "id": "qgd3ySjoW4Wl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = cluster_features(features)"
      ],
      "metadata": {
        "id": "HQmrDuqGW9Un"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_clustered_images(images, labels)"
      ],
      "metadata": {
        "id": "ZjasBVn4XG9i"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}